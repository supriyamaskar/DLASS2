{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Google Colab Lab Assignment -Pretrained Model\n",
        "\n",
        "**Course Name:** [Deep Learning Lab]\n",
        "\n",
        "**Lab Title:** Tomato crop disease classification using pre-trained deep learning\n",
        "algorithm\n",
        "\n",
        "**Student Name:**[Supriya Maskar]\n",
        "\n",
        "**Student ID:**[202201040049]\n",
        "\n",
        "**Date of Submission:** [24/02/2025]\n",
        "\n",
        "**Group Members**: [1)Sapna Dahikamble\n",
        "                  2)Manjiri Netankar]\n",
        "\n",
        "\n",
        "**Research Paper Study and Implementation**\n",
        "\n",
        "**Dataset Link**:https://www.kaggle.com/datasets/kaustubhb999/tomatolea\n",
        "\n",
        "**GitHub Link**:\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CWnjXJ0oa7ru"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Introduction**\n",
        "\n",
        " * This project aims to classify tomato crop diseases using a pre-trained deep\n",
        " * The research paper used as reference is 'Tomato Crop Disease Classification Using Pre-Trained Deep Learning Algorithm and learning model.\n",
        "\n",
        "* The research paper used as reference is 'Tomato Crop Disease Classification Using Pre-Trained Deep Learning Algorithm'."
      ],
      "metadata": {
        "id": "qPt2Z_6fbHnz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 1: Research Paper Selection and Dataset Preparation**\n",
        "* Selected Research Paper: 'Tomato Crop Disease Classification Using Pre-Trained Deep Learning Algorithm'\n",
        "* Pre-trained Model Used: AlexNet\n",
        "* Dataset Used: Tomato Leaf Dataset (https://www.kaggle.com/datasets/kaustubhb999/tomatoleaf)\n",
        "* The dataset contains images of tomato leaves categorized into six disease classes and one healthy class."
      ],
      "metadata": {
        "id": "V1gNgXsJbMnR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "S7thVmJgbTqB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x2zWJgOIa5Rz"
      },
      "outputs": [],
      "source": [
        "# !pip install tensorflow\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import MobileNet\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import zipfile\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "rNya2b9hbaBo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "zip_path = \"/content/archive (12).zip\"  # Updated path as per user's request\n",
        "extract_path = \"/content/tomato_dataset\"\n",
        "\n",
        "if os.path.exists(zip_path):\n",
        "    try:\n",
        "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(extract_path)\n",
        "        print(\"Dataset extracted successfully!\")\n",
        "    except zipfile.BadZipFile:\n",
        "        print(\"Error: The file is not a valid ZIP file. Please re-upload.\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Could not find file at path {zip_path}. Check the file name and path.\")\n",
        "else:\n",
        "    print(\"Error: File not found. Check the file name and path.\")\n",
        "    extracted_dir_name = \"\"\n",
        "for item in os.listdir(extract_path):\n",
        "    if os.path.isdir(os.path.join(extract_path, item)):\n",
        "        extracted_dir_name = item\n",
        "        break\n",
        "\n",
        "if extracted_dir_name:\n",
        "    TRAIN_DIR = os.path.join(extract_path, extracted_dir_name, \"train\") # Update path, add the name of the folder that was extracted\n",
        "    VAL_DIR = os.path.join(extract_path, extracted_dir_name, \"val\") # Update path, add the name of the folder that was extracted\n",
        "    # Verify the existence of the train and val directories\n",
        "    if os.path.exists(TRAIN_DIR) and os.path.exists(VAL_DIR):\n",
        "        print(f\"Train directory: {TRAIN_DIR}\")\n",
        "        print(f\"Validation directory: {VAL_DIR}\")\n",
        "    else:\n",
        "        print(\"Error: train or val directory not found in the extracted folder. Check the zipfile for correct folder structure\")\n",
        "        exit() # Exit because the rest of the code depends on these directories\n",
        "else:\n",
        "    print(\"Error: Could not find the extracted directory. Make sure it exists and is a directory.\")\n",
        "    exit() # Exit because the rest of the code depends on these directories"
      ],
      "metadata": {
        "id": "Gvq7XFnObamq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_DIR = os.path.join(extract_path, \"train\")\n",
        "VAL_DIR = os.path.join(extract_path, \"val\")"
      ],
      "metadata": {
        "id": "pWP-hA1Jbg8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "image_size = (227, 227)  # Required for AlexNet\n",
        "batch_size = 32\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "train_data = datagen.flow_from_directory(\n",
        "    TRAIN_DIR,\n",
        "    target_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "val_data = datagen.flow_from_directory(\n",
        "    VAL_DIR,\n",
        "    target_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")\n"
      ],
      "metadata": {
        "id": "YEpI4Mk0bgtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "base_model = MobileNet(weights='imagenet', include_top=False, input_shape=(227, 227, 3))\n",
        "for layer in base_model.layers[:15]:\n",
        "    layer.trainable = False  # Freeze first 15 layers\n",
        "\n",
        "\n",
        "x = Flatten()(base_model.output)\n",
        "x = Dense(512, activation='relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(256, activation='relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "output_layer = Dense(len(train_data.class_indices), activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=output_layer)"
      ],
      "metadata": {
        "id": "9uHL7OxSbxq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = Flatten()(base_model.output)\n",
        "x = Dense(512, activation='relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(256, activation='relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "output_layer = Dense(len(train_data.class_indices), activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=output_layer)"
      ],
      "metadata": {
        "id": "YDlISmhAb2Fe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 3: Model Evaluation and Performance Comparison**\n",
        "\n",
        "**Instructions:**\n",
        "\n",
        "1. Evaluate the trained model using performance metrics:\n",
        "\n",
        " Accuracy, Precision,Recall, F1-score, Confusion Matrix (for classification tasks)\n",
        "\n",
        "2. Compare the results with those reported in the research paper.\n",
        "\n",
        "3. Identify potential weaknesses and suggest improvements.\n"
      ],
      "metadata": {
        "id": "g7ET2rskb8d4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "6S-tFjUJb8ZI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])history = model.fit(\n",
        "    train_data,\n",
        "    epochs=10,\n",
        "    validation_data=val_data\n",
        ")"
      ],
      "metadata": {
        "id": "XsCCo3pVcCdK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_range = range(10)\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend()\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9DKg-L2-b70t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"tomato_disease_alexnet.h5\")"
      ],
      "metadata": {
        "id": "tTxMUztAcNgN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion:\n",
        "* The dataset was successfully downloaded and preprocessed.\n",
        "* Data augmentation was applied to improve model generalization.\n",
        "* The dataset is ready for model training."
      ],
      "metadata": {
        "id": "f4p13J5rcU_d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Declaration**\n",
        "\n",
        "I, [Supriya Maskar], confirm that the work submitted in this assignment is my own and has been completed following academic integrity guidelines. The code is uploaded on my GitHub repository account, and the repository link is provided below:\n",
        "\n",
        "GitHub Repository Link: [Insert GitHub Link]\n",
        "\n",
        "Signature: [Supriya Maskar]"
      ],
      "metadata": {
        "id": "gND3ZiaGcdCB"
      }
    }
  ]
}